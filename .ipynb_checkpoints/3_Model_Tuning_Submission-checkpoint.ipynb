{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv('./datasets/ames_v1.csv', keep_default_na=False)\n",
    "ames1 = pd.read_csv('datasets/amesv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames1\n",
    "y = np.log(ames['saleprice'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8461452541116159\n",
      "0.9206428930274038\n",
      "0.8621985777386981\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "train_pred = lr.predict(X_train)\n",
    "test_pred = lr.predict(X_test)\n",
    "print(cross_val_score(lr, X, y, cv=5).mean())\n",
    "print(lr.score(X_train,y_train))\n",
    "print(lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.012128184374282612\n",
      "Train RMSE: 0.11012803627724692\n",
      "Train RMSE exp: 19867.57883620927\n",
      "\n",
      "Test MSE: 0.021809584590528973\n",
      "Test RMSE: 0.1476806845546464\n",
      "Test RMSE exp: 26369.70283862772\n"
     ]
    }
   ],
   "source": [
    "train_mse = mean_squared_error(y_true=y_train,y_pred=train_pred)\n",
    "train_rmse = mean_squared_error(y_true=y_train,y_pred=train_pred, squared = False)\n",
    "train_rmse_e = mean_squared_error(y_true=np.exp(y_train),y_pred=np.exp(train_pred), squared = False)\n",
    "\n",
    "test_mse = mean_squared_error(y_true=y_test,y_pred=test_pred)\n",
    "test_rmse = mean_squared_error(y_true=y_test,y_pred=test_pred, squared = False)\n",
    "test_rmse_e = mean_squared_error(y_true=np.exp(y_test),y_pred=np.exp(test_pred), squared = False)\n",
    "\n",
    "print(f'Train MSE: {train_mse}')\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Train RMSE exp: {train_rmse_e}')\n",
    "print()\n",
    "print(f'Test MSE: {test_mse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "print(f'Test RMSE exp: {test_rmse_e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "Z_train = sc.fit_transform(X_train)\n",
    "Z_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sc = LinearRegression()\n",
    "lr_sc.fit(Z_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline R2 score: 0.8461452541116159\n",
      "Train R2 Score: 0.9206428930278018\n",
      "Testing R2 Score: 0.8621985776497884\n"
     ]
    }
   ],
   "source": [
    "print(f'Baseline R2 score: {cross_val_score(lr_sc,X,y).mean()}')\n",
    "print(f'Train R2 Score: {lr_sc.score(Z_train,y_train)}')\n",
    "print(f'Testing R2 Score: {lr_sc.score(Z_test,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression  \n",
    "\n",
    "Using the Ridge Regression, our train R2 score became 91.6% and the test score became 87.9%. There was a better bias-variance trade off compared to the original OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916635693043739\n",
      "0.878658536424019\n"
     ]
    }
   ],
   "source": [
    "ridge_model = Ridge(alpha = 10)\n",
    "ridge_model.fit(Z_train, y_train)\n",
    "\n",
    "print(ridge_model.score(Z_train, y_train))\n",
    "print(ridge_model.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_alphas = np.logspace(-4,0,1000)\n",
    "\n",
    "ridge_cv = RidgeCV(alphas = ridge_alphas, scoring='r2', cv=5)\n",
    "\n",
    "ridge_cv.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train_pred = ridge_cv.predict(Z_train)\n",
    "r_test_pred = ridge_cv.predict(Z_test)\n",
    "\n",
    "# Calculating metrics for the scaled train data\n",
    "r_alpha = ridge_cv.alpha_\n",
    "\n",
    "\n",
    "#ridge train metrics\n",
    "r_train_score = ridge_cv.score(Z_train, y_train)\n",
    "mse_train = metrics.mean_squared_error(y_train, r_train_pred)\n",
    "rmse_train = metrics.mean_squared_error(y_train, r_train_pred, squared=False)\n",
    "exp_train_rmse = metrics.mean_squared_error(np.exp(y_train), np.exp(r_train_pred), squared=False)\n",
    "#ridge test metrics\n",
    "r_test_score = ridge_cv.score(Z_test, y_test)\n",
    "mse_test = metrics.mean_squared_error(y_test, r_test_pred)\n",
    "rmse_test = metrics.mean_squared_error(y_test, r_test_pred, squared=False)\n",
    "exp_test_rmse = metrics.mean_squared_error(np.exp(y_test), np.exp(r_test_pred), squared=False)\n",
    "\n",
    "\n",
    "print(f'Ridge Alpha: {r_alpha}')\n",
    "print(f'Ridge Train R2: {r_train_score}')\n",
    "print(f'Train MSE: {mse_train}')\n",
    "print(f'Train RMSE: {rmse_train}')\n",
    "print(f'Train exp RMSE: {exp_train_rmse}')\n",
    "print()\n",
    "print(f'Test RMSE: {rmse_test}')\n",
    "print(f'Ridge Test R2: {r_test_score}')\n",
    "print(f'Test MSE: {mse_test}')\n",
    "print(f'Test RMSE: {rmse_test}')\n",
    "print(f'Test exp RMSE: {exp_test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Ridge regression holds the normality (of residuals) assumpion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "ridge_resids = y_test - ridge_cv.predict(Z_test)\n",
    "sns.histplot(ridge_resids, bins=20)\n",
    "plt.title('Normally Distributed Residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression: Homoscedasticity of Errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x=ridge_cv.predict(Z_test), y=ridge_resids)\n",
    "plt.axhline(0, color='red')\n",
    "plt.title('Homoscedasticity of Errors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alphas = np.logspace(-3,0,2000)\n",
    "    \n",
    "lasso_cv = LassoCV(alphas = lasso_alphas, cv=5)\n",
    "    \n",
    "lasso_cv.fit(Z_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train_preds = lasso_cv.predict(Z_train)\n",
    "l_test_preds = lasso_cv.predict(Z_test)\n",
    "\n",
    "l_alpha = lasso_cv.alpha_\n",
    "l_train_score = lasso_cv.score(Z_train, y_train)\n",
    "l_test_score = lasso_cv.score(Z_test, y_test)\n",
    "\n",
    "mse_train = metrics.mean_squared_error(y_train, l_train_preds)\n",
    "mse_test = metrics.mean_squared_error(y_test, l_test_preds)\n",
    "\n",
    "rmse_train = metrics.mean_squared_error(y_train, l_train_preds, squared=False)\n",
    "exp_rmse_trn = metrics.mean_squared_error(np.exp(y_train), np.exp(l_train_preds), squared=False)\n",
    "rmse_test = metrics.mean_squared_error(y_test, l_test_preds, squared=False)\n",
    "exp_rmse_tst = metrics.mean_squared_error(np.exp(y_test), np.exp(l_test_preds), squared=False)\n",
    "\n",
    "print(f'Lasso Alpha: {l_alpha}')\n",
    "print(f'Lasso Train R2: {l_train_score}')\n",
    "print(f'Train MSE: {mse_train}')\n",
    "print(f'Train RMSE: {rmse_train}')\n",
    "print(f'Train Exp RMSE: {exp_rmse_trn}')\n",
    "print()\n",
    "print(f'Lasso RMSE: {rmse_train}')\n",
    "print(f'Lasso Test R2: {r_test_score}')\n",
    "print(f'Test MSE: {mse_test}')\n",
    "print(f'Test RMSE: {rmse_test}')\n",
    "print(f'Test Exp RMSE: {exp_rmse_tst}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LASSO regression model shows the normality of residuals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "resids = y_test - lasso_cv.predict(Z_test)\n",
    "sns.histplot(resids, bins=20)\n",
    "plt.title('Normally Distributed Residuals');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The scatterplot below shows the equal variances (of residuals)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x=lasso_cv.predict(Z_test), y=resids)\n",
    "plt.axhline(0, color='red')\n",
    "plt.title('Homoscedasticity of Errors');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learned this function from Ani during the hackathon\n",
    "\n",
    "variable, coef = (X.columns, np.round(lasso_cv.coef_, 3))\n",
    "lasso_coef = pd.DataFrame(data = {\"variable\": variable, \"coef\": coef})\n",
    "lasso_coef = lasso_coef[lasso_coef['coef'] !=0]\n",
    "lasso_coef.sort_values(by = \"coef\", ascending = False)\n",
    "lasso_coef = lasso_coef.sort_values(by = \"coef\", ascending = False)\n",
    "lasso_coef\n",
    "lasso_coef.to_csv('datasets/coef.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this are the optimized features after the lasso regression\n",
    "features = lasso_coef[['variable']]\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model to predict home sale prices in Ames, Iowa was the Ridge model. This model had the best bias-variance tradeoff with a mean train RMSE of 19866 and a mean test RMSE of 25848. The R2 scores show that our variance in our predictions can be explained by 90% in our train dataset and 87% in our test dataset.  \n",
    "\n",
    "The Pearson correlation matrix was utilized to filter dependence among the numerical predictor variables. High coefficients within variables in the Pearson correlation analysis were removed. The distribution of the target variable (Sale Price of Homes) were not normally distributed. A log transformation of the target variable had to be conducted in order to have a normal distribution of our target variable. After creating an initial model, the Ridge and LASSO regularization methods were utilized to provide a better bias-variance trade off.\n",
    "\n",
    "The model will be useful for real estate agents and home sellers/buyers as they evaluate their home values against other homes in Ames, Iowa. The model does better than the baseline model of seeing the average home prices. The baseline model had approximately 80000 in mean error compared to the LASSO model which had a mean error of 23000 in sale price prediction.\n",
    "\n",
    "**Recommendations**  \n",
    "\n",
    "Even with the Ridge model, there is still evidence that the model is overfit. In order to move this project forward, one can reduce complexity by reducing the number of categorical features that were selected in the model. A Spearman correlation analysis can be used to reduce the complexity in the model. The coefficients of the predictor variables could have been analyzed in the initial OLS model to take out any variables that didn't have a strong correlation in predicting the sale price.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearman correlation analysis: https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test['id'].to_frame()\n",
    "test.drop(columns = 'id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have train columns match test columns\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "category = ['object']\n",
    "df_num = test.select_dtypes(include=numerics)\n",
    "df_cat = test.select_dtypes(include=category)\n",
    "n_vars = df_num.columns\n",
    "c_vars = df_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummify test categories\n",
    "test_d = pd.get_dummies(test[c_vars], columns = c_vars, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set polynomial features in test df\n",
    "features = test[n_vars].columns\n",
    "X = test[features]\n",
    "poly = PolynomialFeatures(degree = 2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_df = pd.DataFrame(X_poly, columns = poly.get_feature_names(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine test category dummies and polynomial features\n",
    "test1 = pd.concat([X_poly_df, test_d],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure train and test data sets match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = set(test1.columns) - set(ames1.columns)\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = set(ames1.columns) - set(test1.columns)\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set is now cleaned set model\n",
    "\n",
    "X = ames1\n",
    "y = np.log(ames['saleprice'])\n",
    "\n",
    "lr=LinearRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "train_pred = lr.predict(X)\n",
    "train_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lr.predict(test1)\n",
    "np.log(test_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames1\n",
    "y = np.log(ames['saleprice'])\n",
    "\n",
    "sc = StandardScaler()\n",
    "Z_train = sc.fit_transform(X)\n",
    "Z_test = sc.transform(test1)\n",
    "\n",
    "lasso_alphas = np.logspace(-3,0,2000)\n",
    "    \n",
    "lasso_cv = LassoCV(alphas = lasso_alphas, cv=5)\n",
    "    \n",
    "lasso_cv.fit(Z_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_train_preds = lasso_cv.predict(Z_train)\n",
    "l_test_preds = lasso_cv.predict(Z_test)\n",
    "\n",
    "l_alpha = lasso_cv.alpha_\n",
    "l_train_score = lasso_cv.score(Z_train, y)\n",
    "\n",
    "mse_train = metrics.mean_squared_error(y, l_train_preds)\n",
    "rmse_train = metrics.mean_squared_error(y, l_train_preds, squared=False)\n",
    "\n",
    "print(f'Lasso Alpha: {l_alpha}')\n",
    "print(f'Lasso Train R2: {l_train_score}')\n",
    "print(f'Train MSE: {mse_train}')\n",
    "print(f'Train RMSE: {rmse_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_final = np.exp(lasso_cv.predict(Z_test))\n",
    "model_pred_final.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame({'Id': test_id['id'], 'SalePrice': model_pred_final})\n",
    "prediction['SalePrice'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv('datasets/test_prediction2_pearsonlasso.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
